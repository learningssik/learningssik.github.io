---
layout: post
title: "BigQuery ì‹¤í–‰ ì‹œê°„ ì¶”ì • ìë™í™” ì‹œìŠ¤í…œ"
date: 2025-10-28 00:00:00 +0900
categories: tech
tags: [BigQuery, ì‹¤í–‰ ì‹œê°„ ì¶”ì •, Dry Run, ë¡œê·¸ ë³€í™˜ ëª¨ë¸, ìë™í™”, ë°ì´í„° ë¶„ì„, ì¿¼ë¦¬ ìµœì í™”]
excerpt: "BigQuery ì¿¼ë¦¬ì˜ ì‹¤í–‰ ì‹œê°„ì„ ì‚¬ì „ì— ì •í™•í•˜ê²Œ ì¶”ì •í•˜ê¸° ìœ„í•œ ìë™í™” ì‹œìŠ¤í…œìœ¼ë¡œ, Dry Runì„ í™œìš©í•˜ì—¬ ì¿¼ë¦¬ ì‹¤í–‰ ì „ì— ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ì„ ê³„ì‚°í•˜ê³  ê°œë°œìë“¤ì´ ì¿¼ë¦¬ ìµœì í™”ë¥¼ ë¯¸ë¦¬ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤."
---

# BigQuery ì‹¤í–‰ ì‹œê°„ ì¶”ì • ìë™í™” ì‹œìŠ¤í…œ

## ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” **BigQuery** ì¿¼ë¦¬ì˜ ì‹¤í–‰ ì‹œê°„ì„ ì‚¬ì „ì— ì •í™•í•˜ê²Œ ì¶”ì •í•˜ê¸° ìœ„í•œ ìë™í™” ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ê¸°ì¡´ì—ëŠ” ì¿¼ë¦¬ ì‹¤í–‰ í›„ì—ì•¼ ì‹¤ì œ ì‹¤í–‰ ì‹œê°„ì„ ì•Œ ìˆ˜ ìˆì—ˆì§€ë§Œ, ì´ ì‹œìŠ¤í…œì„ í†µí•´ **Dry Run**ì„ í™œìš©í•˜ì—¬ ì¿¼ë¦¬ ì‹¤í–‰ ì „ì— ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê°œë°œìë“¤ì´ ì¿¼ë¦¬ ìµœì í™”ë¥¼ ë¯¸ë¦¬ ìˆ˜í–‰í•˜ê³ , ë¦¬ì†ŒìŠ¤ ê³„íšì„ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜ë¦½í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.

### í•µì‹¬ íŠ¹ì§•
- **Dry Run ê¸°ë°˜ ì˜ˆì¸¡**: ì‹¤ì œ ì¿¼ë¦¬ ì‹¤í–‰ ì—†ì´ ì‹¤í–‰ ì‹œê°„ ì¶”ì •
- **ë¡œê·¸ ë³€í™˜ ëª¨ë¸**: ì‹¤ì œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì •í™•í•œ ì˜ˆì¸¡ ëª¨ë¸
- **ìë™í™”ëœ ë°ì´í„° ìˆ˜ì§‘**: BigQuery ì‘ì—… ê¸°ë¡ì„ ìë™ìœ¼ë¡œ ìˆ˜ì§‘ ë° ë¶„ì„
- **ì‹œê°í™” ì§€ì›**: ì‹¤í–‰ ì‹œê°„ê³¼ ë°ì´í„° ì²˜ë¦¬ëŸ‰ ê°„ì˜ ìƒê´€ê´€ê³„ ì‹œê°í™”

## ë¬¸ì œ í•´ê²°

### ê¸°ì¡´ ë¬¸ì œì 
1. **ì‹¤í–‰ ì‹œê°„ ì˜ˆì¸¡ ë¶ˆê°€**: ì¿¼ë¦¬ ì‹¤í–‰ ì „ì— ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ì„ ì•Œ ìˆ˜ ì—†ìŒ
2. **ë¦¬ì†ŒìŠ¤ ê³„íš ì–´ë ¤ì›€**: ì˜ˆìƒì¹˜ ëª»í•œ ê¸´ ì‹¤í–‰ ì‹œê°„ìœ¼ë¡œ ì¸í•œ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„
3. **ì¿¼ë¦¬ ìµœì í™” ì§€ì—°**: ì‹¤í–‰ í›„ì—ì•¼ ì„±ëŠ¥ ë¬¸ì œë¥¼ ë°œê²¬í•˜ì—¬ ìµœì í™” ì§€ì—°
4. **ë¹„ìš© ê´€ë¦¬ ì–´ë ¤ì›€**: ì˜ˆìƒì¹˜ ëª»í•œ BigQuery ë¹„ìš© ë°œìƒ
5. **ì‘ì—… ìŠ¤ì¼€ì¤„ë§ ì–´ë ¤ì›€**: ì‹¤í–‰ ì‹œê°„ì„ ëª¨ë¥´ë©´ ì‘ì—… ìˆœì„œ ê³„íšì´ ì–´ë ¤ì›€

### í•´ê²° íš¨ê³¼
- **ì‚¬ì „ ìµœì í™”**: ì¿¼ë¦¬ ì‹¤í–‰ ì „ì— ì„±ëŠ¥ ë¬¸ì œë¥¼ ë¯¸ë¦¬ íŒŒì•…í•˜ê³  ìµœì í™”
- **ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±**: ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ì„ ë°”íƒ•ìœ¼ë¡œ íš¨ìœ¨ì ì¸ ë¦¬ì†ŒìŠ¤ ë°°ì¹˜
- **ë¹„ìš© ì ˆê°**: ë¶ˆí•„ìš”í•œ ê¸´ ì‹¤í–‰ ì‹œê°„ì„ ë°©ì§€í•˜ì—¬ BigQuery ë¹„ìš© ì ˆì•½
- **ì‘ì—… ê³„íš ê°œì„ **: ì‹¤í–‰ ì‹œê°„ ì˜ˆì¸¡ì„ í†µí•œ íš¨ìœ¨ì ì¸ ì‘ì—… ìŠ¤ì¼€ì¤„ë§
- **ê°œë°œ ìƒì‚°ì„± í–¥ìƒ**: ë¹ ë¥¸ í”¼ë“œë°±ì„ í†µí•œ ì¿¼ë¦¬ ê°œë°œ ì†ë„ í–¥ìƒ

## êµ¬í˜„ ë°©ì‹

### 1. ì„œë¹„ìŠ¤ ê³„ì • ë° í™˜ê²½ ì„¤ì •

#### ì„œë¹„ìŠ¤ ê³„ì • í™•ì¸
```python
import requests

def get_service_accounts():
    """Google Cloud VMì—ì„œ ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    # ë©”íƒ€ë°ì´í„° ì„œë²„ì˜ URL
    metadata_url = "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/"
    headers = {"Metadata-Flavor": "Google"}
    
    # ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    response = requests.get(metadata_url, headers=headers)
    
    if response.status_code == 200:
        service_accounts = response.text
        print("Service Accounts:", service_accounts)
        return service_accounts
    else:
        print(f"Failed to retrieve service accounts. Status code: {response.status_code}")
        return None

# ì‚¬ìš© ì˜ˆì‹œ
service_accounts = get_service_accounts()
```

#### BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
```python
from google.cloud import bigquery

def initialize_bigquery_client():
    """BigQuery í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  í˜„ì¬ ì‚¬ìš©ì í™•ì¸"""
    client = bigquery.Client()
    
    # í˜„ì¬ ì‚¬ìš©ì í™•ì¸ ì¿¼ë¦¬
    query = "SELECT CURRENT_USER()"
    query_job = client.query(query)
    
    print(f"BigQuery Client: {client}")
    print(f"Query Job: {query_job}")
    
    return client

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = initialize_bigquery_client()
```

### 2. BigQuery ì‘ì—… ê¸°ë¡ ìˆ˜ì§‘ ì‹œìŠ¤í…œ

#### ì‘ì—… ê¸°ë¡ ìë™ ìˆ˜ì§‘
```python
from google.cloud import bigquery
from datetime import datetime, timedelta
import pandas as pd

def collect_bigquery_jobs(client, service_account_email, limit=500):
    """BigQuery ì‘ì—… ê¸°ë¡ì„ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜"""
    
    def replace_special_chars(input_string):
        """ì¿¼ë¦¬ ë¬¸ìì—´ì—ì„œ íŠ¹ìˆ˜ ë¬¸ìë¥¼ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
        return input_string.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
    
    # ì •ë³´ ìŠ¤í‚¤ë§ˆ ì¿¼ë¦¬ ì‘ì„±
    query = f"""
        SELECT
            cache_hit
            , user_email
            , project_id
            , start_time
            , creation_time
            , end_time
            , TIMESTAMP_DIFF(end_time, creation_time, MILLISECOND)/1000 AS execution_time_seconds
            , total_bytes_processed/1000000 AS execution_total_MB
            , query
        FROM
            region-asia-northeast3.INFORMATION_SCHEMA.JOBS
        WHERE
            1=1
            AND statement_type = 'SELECT'
            AND cache_hit = False
            AND user_email = '{service_account_email}'
            AND query not like '%--%'
            AND query not like '%#%'
            AND query not like '%INFORMATION_SCHEMA%'
        ORDER BY creation_time desc
        LIMIT {limit}
    """
    
    # ì¿¼ë¦¬ ì‹¤í–‰
    query_job = client.query(query)
    
    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    df = query_job.to_dataframe()
    
    # ë°ì´í„° ì •ë¦¬
    df['query'] = df['query'].apply(replace_special_chars)
    df = df.dropna(subset=['execution_time_seconds'])
    df = df.dropna(subset=['execution_total_MB'])
    
    return df

# ì‚¬ìš© ì˜ˆì‹œ
service_account_email = "sa-dev-sa-sip-vm-user@gcp-dev-edp-sa-sip.iam.gserviceaccount.com"
df = collect_bigquery_jobs(client, service_account_email, limit=500)
print(f"ìˆ˜ì§‘ëœ ì‘ì—… ìˆ˜: {len(df)}")
```

### 3. Dry Run ê¸°ë°˜ ì‹¤í–‰ ì‹œê°„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ

#### Dry Run í•¨ìˆ˜ êµ¬í˜„
```python
def dry_run_query(client, query):
    """ì¿¼ë¦¬ì˜ Dry Runì„ ìˆ˜í–‰í•˜ì—¬ ì²˜ë¦¬í•  ë°ì´í„° ì–‘ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜"""
    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)
    
    # Dry Run ì¿¼ë¦¬ ì‹¤í–‰
    query_job = client.query(query, job_config=job_config)
    
    # ì²˜ë¦¬í•  ë°ì´í„° ì–‘ì„ MB ë‹¨ìœ„ë¡œ ë°˜í™˜
    return query_job.total_bytes_processed / 1000000

def batch_dry_run(df, client):
    """ë°ì´í„°í”„ë ˆì„ì˜ ëª¨ë“  ì¿¼ë¦¬ì— ëŒ€í•´ Dry Runì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜"""
    dry_run_results = []
    
    for idx, row in df.iterrows():
        try:
            dry_run_mb = dry_run_query(client, row['query'])
            dry_run_results.append(dry_run_mb)
            print(f"ì¿¼ë¦¬ {idx+1}/{len(df)} ì²˜ë¦¬ ì™„ë£Œ: {dry_run_mb:.2f} MB")
        except Exception as e:
            print(f"ì¿¼ë¦¬ {idx+1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            dry_run_results.append(None)
    
    df['dry_run_MB'] = dry_run_results
    return df

# ì‚¬ìš© ì˜ˆì‹œ
df_with_dry_run = batch_dry_run(df, client)
df_with_dry_run.to_csv('dry_run.csv', index=False)
```

### 4. ë¡œê·¸ ë³€í™˜ ê¸°ë°˜ ì˜ˆì¸¡ ëª¨ë¸

#### ì‹¤í–‰ ì‹œê°„ ì˜ˆì¸¡ í•¨ìˆ˜
```python
import numpy as np

def predict_execution_time_seconds_log(dry_run_MB):
    """
    Dry Run MB ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ë¡œê·¸ ë³€í™˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰ ì‹œê°„ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜
    
    ì´ í•¨ìˆ˜ëŠ” 60ì´ˆ ì´í•˜ì˜ ì‹¤í–‰ ì‹œê°„ìœ¼ë¡œ í•„í„°ë§ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ
    ë¡œê·¸ ë³€í™˜ ì„ í˜• íšŒê·€ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    Parameters:
    dry_run_MB (float): Dry Runì—ì„œ ì˜ˆì¸¡ëœ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)
    
    Returns:
    float: ì˜ˆì¸¡ëœ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
    """
    log_slope = 1.67
    log_intercept = -7.52
    log_dry_run_MB = np.log1p(dry_run_MB)  # log(1 + x) ë³€í™˜
    return log_slope * log_dry_run_MB + log_intercept

def predict_execution_time_batch(dry_run_values):
    """ì—¬ëŸ¬ Dry Run ê°’ì— ëŒ€í•´ ë°°ì¹˜ë¡œ ì‹¤í–‰ ì‹œê°„ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜"""
    predictions = []
    for dry_run_mb in dry_run_values:
        if dry_run_mb is not None and dry_run_mb > 0:
            prediction = predict_execution_time_seconds_log(dry_run_mb)
            predictions.append(prediction)
        else:
            predictions.append(None)
    return predictions

# ì‚¬ìš© ì˜ˆì‹œ
dry_run_mb = 100
predicted_time = predict_execution_time_seconds_log(dry_run_mb)
print(f"Dry Run: {dry_run_mb} MB -> ì˜ˆì¸¡ ì‹¤í–‰ ì‹œê°„: {predicted_time:.2f} ì´ˆ")

# ë°°ì¹˜ ì˜ˆì¸¡
df['predicted_execution_time'] = predict_execution_time_batch(df['dry_run_MB'])
```

### 5. ë°ì´í„° ì‹œê°í™” ë° ë¶„ì„ ì‹œìŠ¤í…œ

#### ìƒê´€ê´€ê³„ ì‹œê°í™”
```python
import matplotlib.pyplot as plt

def visualize_execution_correlation(df):
    """ì‹¤í–‰ ì‹œê°„ê³¼ ë°ì´í„° ì²˜ë¦¬ëŸ‰ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜"""
    
    # ë°ì´í„° ì •ë¦¬ (None ê°’ ì œê±°)
    clean_df = df.dropna(subset=['execution_total_MB', 'execution_time_seconds'])
    
    # ê·¸ë˜í”„ ì„¤ì •
    plt.figure(figsize=(12, 8))
    
    # ì‹¤ì œ ì‹¤í–‰ ì‹œê°„ vs ë°ì´í„° ì²˜ë¦¬ëŸ‰
    plt.subplot(2, 2, 1)
    plt.scatter(clean_df['execution_total_MB'], clean_df['execution_time_seconds'], 
                alpha=0.6, color='blue')
    plt.xlabel('ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ëŸ‰ (MB)')
    plt.ylabel('ì‹¤ì œ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)')
    plt.title('ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ëŸ‰ vs ì‹¤í–‰ ì‹œê°„')
    plt.grid(True)
    
    # Dry Run vs ì‹¤ì œ ì‹¤í–‰ ì‹œê°„
    plt.subplot(2, 2, 2)
    clean_dry_run = df.dropna(subset=['dry_run_MB', 'execution_time_seconds'])
    plt.scatter(clean_dry_run['dry_run_MB'], clean_dry_run['execution_time_seconds'], 
                alpha=0.6, color='green')
    plt.xlabel('Dry Run ì˜ˆì¸¡ ë°ì´í„°ëŸ‰ (MB)')
    plt.ylabel('ì‹¤ì œ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)')
    plt.title('Dry Run ì˜ˆì¸¡ vs ì‹¤ì œ ì‹¤í–‰ ì‹œê°„')
    plt.grid(True)
    
    # ì˜ˆì¸¡ ì‹œê°„ vs ì‹¤ì œ ì‹œê°„
    plt.subplot(2, 2, 3)
    clean_prediction = df.dropna(subset=['predicted_execution_time', 'execution_time_seconds'])
    plt.scatter(clean_prediction['predicted_execution_time'], 
                clean_prediction['execution_time_seconds'], 
                alpha=0.6, color='red')
    plt.xlabel('ì˜ˆì¸¡ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)')
    plt.ylabel('ì‹¤ì œ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)')
    plt.title('ì˜ˆì¸¡ ì‹œê°„ vs ì‹¤ì œ ì‹œê°„')
    plt.grid(True)
    
    # ì‹œê°„ë³„ íŠ¸ë Œë“œ
    plt.subplot(2, 2, 4)
    plt.plot(clean_df['execution_total_MB'], clean_df['execution_time_seconds'], 
             marker='o', alpha=0.7)
    plt.xlabel('ë°ì´í„° ì²˜ë¦¬ëŸ‰ (MB)')
    plt.ylabel('ì‹¤ì œ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)')
    plt.title('ë°ì´í„° ì²˜ë¦¬ëŸ‰ë³„ ì‹¤í–‰ ì‹œê°„ íŠ¸ë Œë“œ')
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()
    
    return clean_df

# ì‚¬ìš© ì˜ˆì‹œ
visualization_data = visualize_execution_correlation(df)
```

#### ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
```python
def evaluate_model_performance(df):
    """ì˜ˆì¸¡ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜"""
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
    
    # ìœ íš¨í•œ ì˜ˆì¸¡ê°’ë§Œ ì¶”ì¶œ
    valid_data = df.dropna(subset=['predicted_execution_time', 'execution_time_seconds'])
    
    if len(valid_data) == 0:
        print("í‰ê°€í•  ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    actual = valid_data['execution_time_seconds']
    predicted = valid_data['predicted_execution_time']
    
    # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    mae = mean_absolute_error(actual, predicted)
    mse = mean_squared_error(actual, predicted)
    rmse = np.sqrt(mse)
    r2 = r2_score(actual, predicted)
    
    # ê²°ê³¼ ì¶œë ¥
    print("=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===")
    print(f"í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (MAE): {mae:.2f} ì´ˆ")
    print(f"í‰ê·  ì œê³± ì˜¤ì°¨ (MSE): {mse:.2f}")
    print(f"ì œê³±ê·¼ í‰ê·  ì œê³± ì˜¤ì°¨ (RMSE): {rmse:.2f} ì´ˆ")
    print(f"ê²°ì • ê³„ìˆ˜ (RÂ²): {r2:.4f}")
    print(f"í‰ê°€ ë°ì´í„° ìˆ˜: {len(valid_data)}ê°œ")
    
    # ì •í™•ë„ ë¶„ì„
    accuracy_within_10_percent = sum(abs(actual - predicted) / actual <= 0.1) / len(actual)
    accuracy_within_20_percent = sum(abs(actual - predicted) / actual <= 0.2) / len(actual)
    
    print(f"10% ì´ë‚´ ì •í™•ë„: {accuracy_within_10_percent:.2%}")
    print(f"20% ì´ë‚´ ì •í™•ë„: {accuracy_within_20_percent:.2%}")
    
    return {
        'mae': mae,
        'mse': mse,
        'rmse': rmse,
        'r2': r2,
        'accuracy_10': accuracy_within_10_percent,
        'accuracy_20': accuracy_within_20_percent
    }

# ì‚¬ìš© ì˜ˆì‹œ
performance_metrics = evaluate_model_performance(df)
```

### 6. í†µí•© ì‹¤í–‰ ì‹œê°„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ

#### ì™„ì „í•œ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸
```python
class BigQueryExecutionTimePredictor:
    """BigQuery ì‹¤í–‰ ì‹œê°„ ì˜ˆì¸¡ì„ ìœ„í•œ í†µí•© í´ë˜ìŠ¤"""
    
    def __init__(self, service_account_email):
        self.service_account_email = service_account_email
        self.client = bigquery.Client()
        self.model_params = {
            'log_slope': 1.67,
            'log_intercept': -7.52
        }
    
    def predict_single_query(self, query):
        """ë‹¨ì¼ ì¿¼ë¦¬ì˜ ì‹¤í–‰ ì‹œê°„ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜"""
        try:
            # Dry Run ìˆ˜í–‰
            dry_run_mb = dry_run_query(self.client, query)
            
            # ì‹¤í–‰ ì‹œê°„ ì˜ˆì¸¡
            predicted_time = predict_execution_time_seconds_log(dry_run_mb)
            
            return {
                'query': query,
                'dry_run_mb': dry_run_mb,
                'predicted_execution_time_seconds': predicted_time,
                'status': 'success'
            }
        except Exception as e:
            return {
                'query': query,
                'error': str(e),
                'status': 'error'
            }
    
    def predict_multiple_queries(self, queries):
        """ì—¬ëŸ¬ ì¿¼ë¦¬ì˜ ì‹¤í–‰ ì‹œê°„ì„ ë°°ì¹˜ë¡œ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜"""
        results = []
        
        for i, query in enumerate(queries):
            print(f"ì¿¼ë¦¬ {i+1}/{len(queries)} ì²˜ë¦¬ ì¤‘...")
            result = self.predict_single_query(query)
            results.append(result)
        
        return results
    
    def get_historical_data(self, limit=500):
        """ê³¼ê±° ì‘ì—… ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜"""
        return collect_bigquery_jobs(self.client, self.service_account_email, limit)
    
    def retrain_model(self, df):
        """ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ì¬í›ˆë ¨í•˜ëŠ” í•¨ìˆ˜"""
        # ë¡œê·¸ ë³€í™˜ ì„ í˜• íšŒê·€ ëª¨ë¸ ì¬í›ˆë ¨
        from sklearn.linear_model import LinearRegression
        
        # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì¶”ì¶œ
        valid_data = df.dropna(subset=['dry_run_MB', 'execution_time_seconds'])
        
        if len(valid_data) < 10:
            print("ì¬í›ˆë ¨ì„ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ë¡œê·¸ ë³€í™˜
        X = np.log1p(valid_data['dry_run_MB']).values.reshape(-1, 1)
        y = valid_data['execution_time_seconds'].values
        
        # ëª¨ë¸ í›ˆë ¨
        model = LinearRegression()
        model.fit(X, y)
        
        # ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„° ì €ì¥
        self.model_params['log_slope'] = model.coef_[0]
        self.model_params['log_intercept'] = model.intercept_
        
        print(f"ëª¨ë¸ ì¬í›ˆë ¨ ì™„ë£Œ:")
        print(f"ìƒˆë¡œìš´ ê¸°ìš¸ê¸°: {self.model_params['log_slope']:.2f}")
        print(f"ìƒˆë¡œìš´ ì ˆí¸: {self.model_params['log_intercept']:.2f}")
        
        return True

# ì‚¬ìš© ì˜ˆì‹œ
predictor = BigQueryExecutionTimePredictor(service_account_email)

# ë‹¨ì¼ ì¿¼ë¦¬ ì˜ˆì¸¡
sample_query = """
SELECT COUNT(*) 
FROM `your-gcp-project.your-dataset.your-table-name` 
WHERE P_YYYYMMDD BETWEEN '2024-04-01' AND '2024-04-30'
"""
result = predictor.predict_single_query(sample_query)
print(f"ì˜ˆì¸¡ ê²°ê³¼: {result}")
```

## ë¦¬íŒ©í† ë§ ì œì•ˆ

### 1. ëª¨ë¸ ê°œì„  ë° ì •í™•ë„ í–¥ìƒ

#### ê³ ê¸‰ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ë„ì…
```python
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
import joblib

class AdvancedExecutionTimePredictor:
    """ê³ ê¸‰ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì‹¤í–‰ ì‹œê°„ ì˜ˆì¸¡ê¸°"""
    
    def __init__(self):
        self.models = {
            'linear': LinearRegression(),
            'random_forest': RandomForestRegressor(n_estimators=100, random_state=42),
            'gradient_boosting': GradientBoostingRegressor(random_state=42)
        }
        self.scaler = StandardScaler()
        self.best_model = None
        self.feature_names = ['log_dry_run_mb', 'query_length', 'table_count', 'join_count']
    
    def extract_features(self, query, dry_run_mb):
        """ì¿¼ë¦¬ì—ì„œ ì¶”ê°€ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
        features = {
            'log_dry_run_mb': np.log1p(dry_run_mb),
            'query_length': len(query),
            'table_count': query.upper().count('FROM'),
            'join_count': query.upper().count('JOIN')
        }
        return [features[name] for name in self.feature_names]
    
    def train_models(self, df):
        """ì—¬ëŸ¬ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ìµœì  ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” í•¨ìˆ˜"""
        # íŠ¹ì„± ì¶”ì¶œ
        X = []
        y = []
        
        for _, row in df.iterrows():
            if pd.notna(row['dry_run_MB']) and pd.notna(row['execution_time_seconds']):
                features = self.extract_features(row['query'], row['dry_run_MB'])
                X.append(features)
                y.append(row['execution_time_seconds'])
        
        X = np.array(X)
        y = np.array(y)
        
        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€
        best_score = -np.inf
        best_model_name = None
        
        for name, model in self.models.items():
            if name == 'linear':
                model.fit(X_train_scaled, y_train)
                score = model.score(X_test_scaled, y_test)
            else:
                model.fit(X_train, y_train)
                score = model.score(X_test, y_test)
            
            print(f"{name} ëª¨ë¸ RÂ² ì ìˆ˜: {score:.4f}")
            
            if score > best_score:
                best_score = score
                best_model_name = name
                self.best_model = model
        
        print(f"ìµœì  ëª¨ë¸: {best_model_name} (RÂ² = {best_score:.4f})")
        
        # êµì°¨ ê²€ì¦
        if best_model_name == 'linear':
            cv_scores = cross_val_score(self.best_model, X_train_scaled, y_train, cv=5)
        else:
            cv_scores = cross_val_score(self.best_model, X_train, y_train, cv=5)
        
        print(f"êµì°¨ ê²€ì¦ í‰ê·  ì ìˆ˜: {cv_scores.mean():.4f} (Â±{cv_scores.std() * 2:.4f})")
        
        return best_model_name, best_score
    
    def predict(self, query, dry_run_mb):
        """ìµœì  ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰ ì‹œê°„ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜"""
        if self.best_model is None:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. train_models()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        features = self.extract_features(query, dry_run_mb)
        
        if hasattr(self.best_model, 'predict'):
            if isinstance(self.best_model, LinearRegression):
                features_scaled = self.scaler.transform([features])
                prediction = self.best_model.predict(features_scaled)[0]
            else:
                prediction = self.best_model.predict([features])[0]
        else:
            prediction = self.best_model.predict([features])[0]
        
        return max(0, prediction)  # ìŒìˆ˜ ì˜ˆì¸¡ ë°©ì§€
    
    def save_model(self, filepath):
        """í›ˆë ¨ëœ ëª¨ë¸ì„ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
        model_data = {
            'best_model': self.best_model,
            'scaler': self.scaler,
            'feature_names': self.feature_names
        }
        joblib.dump(model_data, filepath)
        print(f"ëª¨ë¸ì´ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def load_model(self, filepath):
        """ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
        model_data = joblib.load(filepath)
        self.best_model = model_data['best_model']
        self.scaler = model_data['scaler']
        self.feature_names = model_data['feature_names']
        print(f"ëª¨ë¸ì´ {filepath}ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
```

### 2. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ

#### ì‹¤í–‰ ì‹œê°„ ëª¨ë‹ˆí„°ë§
```python
import time
from datetime import datetime
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

class ExecutionTimeMonitor:
    """ì‹¤í–‰ ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ"""
    
    def __init__(self, predictor, threshold_seconds=300):
        self.predictor = predictor
        self.threshold_seconds = threshold_seconds
        self.monitoring_log = []
    
    def monitor_query_execution(self, query, user_email=None):
        """ì¿¼ë¦¬ ì‹¤í–‰ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ì•Œë¦¼ì„ ë³´ë‚´ëŠ” í•¨ìˆ˜"""
        # ì‹¤í–‰ ì‹œê°„ ì˜ˆì¸¡
        dry_run_mb = dry_run_query(self.predictor.client, query)
        predicted_time = self.predictor.predict_single_query(query)
        
        if predicted_time['status'] == 'error':
            print(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {predicted_time['error']}")
            return None
        
        predicted_seconds = predicted_time['predicted_execution_time_seconds']
        
        # ì„ê³„ê°’ í™•ì¸
        if predicted_seconds > self.threshold_seconds:
            alert_message = self._create_alert_message(query, predicted_seconds, dry_run_mb)
            print(f"âš ï¸ ê²½ê³ : ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ì´ {self.threshold_seconds}ì´ˆë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤!")
            print(f"ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„: {predicted_seconds:.2f}ì´ˆ")
            print(f"ì²˜ë¦¬í•  ë°ì´í„°ëŸ‰: {dry_run_mb:.2f} MB")
            
            # ì•Œë¦¼ ì „ì†¡
            if user_email:
                self._send_alert_email(user_email, alert_message)
            
            # ëª¨ë‹ˆí„°ë§ ë¡œê·¸ì— ê¸°ë¡
            self._log_monitoring_event(query, predicted_seconds, dry_run_mb, 'threshold_exceeded')
        
        return {
            'predicted_time': predicted_seconds,
            'dry_run_mb': dry_run_mb,
            'threshold_exceeded': predicted_seconds > self.threshold_seconds
        }
    
    def _create_alert_message(self, query, predicted_time, dry_run_mb):
        """ì•Œë¦¼ ë©”ì‹œì§€ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
        message = f"""
BigQuery ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ ê²½ê³ 

ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„: {predicted_time:.2f}ì´ˆ
ì²˜ë¦¬í•  ë°ì´í„°ëŸ‰: {dry_run_mb:.2f} MB
ì„ê³„ê°’: {self.threshold_seconds}ì´ˆ

ì¿¼ë¦¬:
{query[:500]}{'...' if len(query) > 500 else ''}

ê¶Œì¥ì‚¬í•­:
1. ì¿¼ë¦¬ ìµœì í™” ê²€í† 
2. íŒŒí‹°ì…˜ í•„í„° ì¶”ê°€
3. ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
4. ì ì ˆí•œ ì¸ë±ìŠ¤ ì‚¬ìš© í™•ì¸
        """
        return message
    
    def _send_alert_email(self, user_email, message):
        """ì´ë©”ì¼ ì•Œë¦¼ì„ ì „ì†¡í•˜ëŠ” í•¨ìˆ˜"""
        try:
            # SMTP ì„¤ì • (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
            smtp_server = "smtp.gmail.com"
            smtp_port = 587
            sender_email = "your-email@gmail.com"
            sender_password = "your-password"
            
            # ì´ë©”ì¼ êµ¬ì„±
            msg = MIMEMultipart()
            msg['From'] = sender_email
            msg['To'] = user_email
            msg['Subject'] = "BigQuery ì‹¤í–‰ ì‹œê°„ ê²½ê³ "
            
            msg.attach(MIMEText(message, 'plain'))
            
            # ì´ë©”ì¼ ì „ì†¡
            server = smtplib.SMTP(smtp_server, smtp_port)
            server.starttls()
            server.login(sender_email, sender_password)
            text = msg.as_string()
            server.sendmail(sender_email, user_email, text)
            server.quit()
            
            print(f"ê²½ê³  ì´ë©”ì¼ì´ {user_email}ë¡œ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"ì´ë©”ì¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
    
    def _log_monitoring_event(self, query, predicted_time, dry_run_mb, event_type):
        """ëª¨ë‹ˆí„°ë§ ì´ë²¤íŠ¸ë¥¼ ë¡œê·¸ì— ê¸°ë¡í•˜ëŠ” í•¨ìˆ˜"""
        event = {
            'timestamp': datetime.now(),
            'query_hash': hash(query),
            'predicted_time': predicted_time,
            'dry_run_mb': dry_run_mb,
            'event_type': event_type
        }
        self.monitoring_log.append(event)
    
    def get_monitoring_summary(self):
        """ëª¨ë‹ˆí„°ë§ ìš”ì•½ ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
        if not self.monitoring_log:
            return "ëª¨ë‹ˆí„°ë§ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        total_events = len(self.monitoring_log)
        threshold_exceeded = sum(1 for event in self.monitoring_log if event['event_type'] == 'threshold_exceeded')
        avg_predicted_time = sum(event['predicted_time'] for event in self.monitoring_log) / total_events
        
        return {
            'total_monitored_queries': total_events,
            'threshold_exceeded_count': threshold_exceeded,
            'threshold_exceeded_rate': threshold_exceeded / total_events,
            'average_predicted_time': avg_predicted_time,
            'last_monitoring_time': self.monitoring_log[-1]['timestamp'] if self.monitoring_log else None
        }
```

### 3. ì¿¼ë¦¬ ìµœì í™” ì œì•ˆ ì‹œìŠ¤í…œ

#### ìë™ ìµœì í™” ì œì•ˆ
```python
import re
from typing import List, Dict, Any

class QueryOptimizationAdvisor:
    """ì¿¼ë¦¬ ìµœì í™” ì œì•ˆì„ ì œê³µí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.optimization_rules = {
            'partition_filter': {
                'pattern': r'WHERE\s+.*?P_YYYYMMDD\s*[<>=]+\s*[\'"]?\d{4}-\d{2}-\d{2}[\'"]?',
                'suggestion': 'íŒŒí‹°ì…˜ í•„í„°ê°€ ì ì ˆíˆ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.',
                'priority': 'high'
            },
            'select_star': {
                'pattern': r'SELECT\s+\*',
                'suggestion': 'SELECT * ëŒ€ì‹  í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ë°ì´í„° ì „ì†¡ëŸ‰ì„ ì¤„ì´ì„¸ìš”.',
                'priority': 'medium'
            },
            'missing_limit': {
                'pattern': r'SELECT.*(?!LIMIT\s+\d+)',
                'suggestion': 'ê²°ê³¼ ì§‘í•©ì´ í´ ê²½ìš° LIMITì„ ì¶”ê°€í•˜ì—¬ ì‹¤í–‰ ì‹œê°„ì„ ë‹¨ì¶•í•˜ì„¸ìš”.',
                'priority': 'low'
            },
            'complex_join': {
                'pattern': r'JOIN.*JOIN.*JOIN',
                'suggestion': 'ë³µì¡í•œ JOINì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ì™€ ì¡°ì¸ ìˆœì„œë¥¼ í™•ì¸í•˜ì„¸ìš”.',
                'priority': 'high'
            }
        }
    
    def analyze_query(self, query: str) -> List[Dict[str, Any]]:
        """ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ê³  ìµœì í™” ì œì•ˆì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
        suggestions = []
        
        for rule_name, rule in self.optimization_rules.items():
            if re.search(rule['pattern'], query, re.IGNORECASE):
                suggestions.append({
                    'rule': rule_name,
                    'suggestion': rule['suggestion'],
                    'priority': rule['priority'],
                    'query_snippet': self._extract_relevant_snippet(query, rule['pattern'])
                })
        
        # ì¶”ê°€ ë¶„ì„
        suggestions.extend(self._analyze_query_structure(query))
        
        return sorted(suggestions, key=lambda x: self._priority_score(x['priority']))
    
    def _extract_relevant_snippet(self, query: str, pattern: str) -> str:
        """ì¿¼ë¦¬ì—ì„œ ê´€ë ¨ ë¶€ë¶„ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
        match = re.search(pattern, query, re.IGNORECASE)
        if match:
            start = max(0, match.start() - 50)
            end = min(len(query), match.end() + 50)
            return query[start:end]
        return ""
    
    def _analyze_query_structure(self, query: str) -> List[Dict[str, Any]]:
        """ì¿¼ë¦¬ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜"""
        suggestions = []
        
        # í…Œì´ë¸” ìˆ˜ ë¶„ì„
        table_count = len(re.findall(r'FROM\s+\w+', query, re.IGNORECASE))
        if table_count > 5:
            suggestions.append({
                'rule': 'many_tables',
                'suggestion': f'{table_count}ê°œì˜ í…Œì´ë¸”ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ì¿¼ë¦¬ë¥¼ ë¶„í• í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”.',
                'priority': 'medium',
                'query_snippet': 'Multiple tables detected'
            })
        
        # ì„œë¸Œì¿¼ë¦¬ ë¶„ì„
        subquery_count = len(re.findall(r'\(SELECT', query, re.IGNORECASE))
        if subquery_count > 3:
            suggestions.append({
                'rule': 'complex_subqueries',
                'suggestion': f'{subquery_count}ê°œì˜ ì„œë¸Œì¿¼ë¦¬ê°€ ìˆìŠµë‹ˆë‹¤. CTEë‚˜ ì„ì‹œ í…Œì´ë¸” ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”.',
                'priority': 'medium',
                'query_snippet': 'Multiple subqueries detected'
            })
        
        return suggestions
    
    def _priority_score(self, priority: str) -> int:
        """ìš°ì„ ìˆœìœ„ë¥¼ ì ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
        priority_scores = {'high': 3, 'medium': 2, 'low': 1}
        return priority_scores.get(priority, 0)
    
    def generate_optimization_report(self, query: str, predicted_time: float) -> str:
        """ìµœì í™” ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
        suggestions = self.analyze_query(query)
        
        report = f"""
=== BigQuery ì¿¼ë¦¬ ìµœì í™” ë³´ê³ ì„œ ===

ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„: {predicted_time:.2f}ì´ˆ

ìµœì í™” ì œì•ˆ:
"""
        
        for i, suggestion in enumerate(suggestions, 1):
            priority_emoji = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}[suggestion['priority']]
            report += f"""
{i}. {priority_emoji} {suggestion['suggestion']}
   ê·œì¹™: {suggestion['rule']}
"""
        
        if not suggestions:
            report += "\ní˜„ì¬ ì¿¼ë¦¬ëŠ” ì˜ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤!"
        
        return report

# ì‚¬ìš© ì˜ˆì‹œ
advisor = QueryOptimizationAdvisor()
sample_query = """
SELECT * FROM `your-gcp-project.your-dataset.your-table-name` 
WHERE P_YYYYMMDD BETWEEN '2024-04-01' AND '2024-04-30'
"""
suggestions = advisor.analyze_query(sample_query)
report = advisor.generate_optimization_report(sample_query, 45.2)
print(report)
```

### 4. ì›¹ ëŒ€ì‹œë³´ë“œ ë° API ì„œë¹„ìŠ¤

#### FastAPI ê¸°ë°˜ REST API
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import uvicorn

app = FastAPI(title="BigQuery Execution Time Predictor API")

class QueryRequest(BaseModel):
    query: str
    user_email: Optional[str] = None

class PredictionResponse(BaseModel):
    predicted_execution_time_seconds: float
    dry_run_mb: float
    threshold_exceeded: bool
    optimization_suggestions: List[dict]

class BatchPredictionRequest(BaseModel):
    queries: List[str]

class BatchPredictionResponse(BaseModel):
    results: List[dict]
    summary: dict

# ì „ì—­ ì˜ˆì¸¡ê¸° ì¸ìŠ¤í„´ìŠ¤
predictor = None
monitor = None
advisor = None

@app.on_event("startup")
async def startup_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global predictor, monitor, advisor
    
    service_account_email = "sa-dev-sa-sip-vm-user@gcp-dev-edp-sa-sip.iam.gserviceaccount.com"
    predictor = BigQueryExecutionTimePredictor(service_account_email)
    monitor = ExecutionTimeMonitor(predictor, threshold_seconds=300)
    advisor = QueryOptimizationAdvisor()
    
    print("BigQuery Execution Time Predictor APIê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")

@app.post("/predict", response_model=PredictionResponse)
async def predict_execution_time(request: QueryRequest):
    """ë‹¨ì¼ ì¿¼ë¦¬ì˜ ì‹¤í–‰ ì‹œê°„ì„ ì˜ˆì¸¡í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # ì‹¤í–‰ ì‹œê°„ ì˜ˆì¸¡
        prediction_result = predictor.predict_single_query(request.query)
        
        if prediction_result['status'] == 'error':
            raise HTTPException(status_code=400, detail=prediction_result['error'])
        
        predicted_time = prediction_result['predicted_execution_time_seconds']
        dry_run_mb = prediction_result['dry_run_mb']
        
        # ëª¨ë‹ˆí„°ë§
        monitor_result = monitor.monitor_query_execution(request.query, request.user_email)
        
        # ìµœì í™” ì œì•ˆ
        suggestions = advisor.analyze_query(request.query)
        
        return PredictionResponse(
            predicted_execution_time_seconds=predicted_time,
            dry_run_mb=dry_run_mb,
            threshold_exceeded=monitor_result['threshold_exceeded'] if monitor_result else False,
            optimization_suggestions=suggestions
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/predict/batch", response_model=BatchPredictionResponse)
async def predict_batch_execution_time(request: BatchPredictionRequest):
    """ì—¬ëŸ¬ ì¿¼ë¦¬ì˜ ì‹¤í–‰ ì‹œê°„ì„ ë°°ì¹˜ë¡œ ì˜ˆì¸¡í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸"""
    try:
        results = []
        total_predicted_time = 0
        threshold_exceeded_count = 0
        
        for query in request.queries:
            prediction_result = predictor.predict_single_query(query)
            
            if prediction_result['status'] == 'success':
                predicted_time = prediction_result['predicted_execution_time_seconds']
                total_predicted_time += predicted_time
                
                if predicted_time > monitor.threshold_seconds:
                    threshold_exceeded_count += 1
                
                results.append({
                    'query': query,
                    'predicted_execution_time_seconds': predicted_time,
                    'dry_run_mb': prediction_result['dry_run_mb'],
                    'threshold_exceeded': predicted_time > monitor.threshold_seconds,
                    'status': 'success'
                })
            else:
                results.append({
                    'query': query,
                    'error': prediction_result['error'],
                    'status': 'error'
                })
        
        summary = {
            'total_queries': len(request.queries),
            'successful_predictions': len([r for r in results if r['status'] == 'success']),
            'total_predicted_time': total_predicted_time,
            'threshold_exceeded_count': threshold_exceeded_count,
            'average_predicted_time': total_predicted_time / len([r for r in results if r['status'] == 'success']) if results else 0
        }
        
        return BatchPredictionResponse(results=results, summary=summary)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/monitoring/summary")
async def get_monitoring_summary():
    """ëª¨ë‹ˆí„°ë§ ìš”ì•½ ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸"""
    try:
        summary = monitor.get_monitoring_summary()
        return summary
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {"status": "healthy", "service": "BigQuery Execution Time Predictor"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## í•™ìŠµ ë‚´ìš©

### ê¸°ìˆ ì  ì¸ì‚¬ì´íŠ¸

#### 1. Dry Runì˜ í™œìš© ê°€ì¹˜
- **ë¹„ìš© íš¨ìœ¨ì„±**: ì‹¤ì œ ì¿¼ë¦¬ ì‹¤í–‰ ì—†ì´ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆì–´ ë¹„ìš© ì ˆì•½
- **ì„±ëŠ¥ ì˜ˆì¸¡**: ì²˜ë¦¬í•  ë°ì´í„° ì–‘ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹¤í–‰ ì‹œê°„ì„ ì •í™•í•˜ê²Œ ì˜ˆì¸¡ ê°€ëŠ¥
- **ì‚¬ì „ ìµœì í™”**: ì‹¤í–‰ ì „ì— ì„±ëŠ¥ ë¬¸ì œë¥¼ íŒŒì•…í•˜ê³  ìµœì í™”í•  ìˆ˜ ìˆëŠ” ê¸°íšŒ ì œê³µ

#### 2. ë¡œê·¸ ë³€í™˜ ëª¨ë¸ì˜ íš¨ê³¼
- **ì„ í˜• ê´€ê³„ ë³€í™˜**: ë¡œê·¸ ë³€í™˜ì„ í†µí•´ ë¹„ì„ í˜• ê´€ê³„ë¥¼ ì„ í˜•ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ë§ ì •í™•ë„ í–¥ìƒ
- **ì´ìƒì¹˜ ì²˜ë¦¬**: ë¡œê·¸ ë³€í™˜ì´ ê·¹ê°’ì˜ ì˜í–¥ì„ ì¤„ì—¬ ëª¨ë¸ì˜ ì•ˆì •ì„± í–¥ìƒ
- **ì‹¤ìš©ì  ì •í™•ë„**: ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€ì˜ ì˜ˆì¸¡ ì •í™•ë„ ë‹¬ì„±

#### 3. ë°ì´í„° ê¸°ë°˜ ëª¨ë¸ë§ì˜ ì¤‘ìš”ì„±
- **ì‹¤ì œ ë°ì´í„° í™œìš©**: ê³¼ê±° ì‹¤í–‰ ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í˜„ì‹¤ì ì¸ ëª¨ë¸ êµ¬ì¶•
- **ì§€ì†ì  ê°œì„ **: ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ í†µí•œ ëª¨ë¸ ì¬í›ˆë ¨ìœ¼ë¡œ ì •í™•ë„ ì§€ì† í–¥ìƒ
- **ë„ë©”ì¸ íŠ¹í™”**: BigQuery íŠ¹ì„±ì— ë§ëŠ” ë§ì¶¤í˜• ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œ

### ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜

#### 1. ë¹„ìš© ì ˆê° íš¨ê³¼
- **ì˜ˆìƒì¹˜ ëª»í•œ ë¹„ìš© ë°©ì§€**: ê¸´ ì‹¤í–‰ ì‹œê°„ìœ¼ë¡œ ì¸í•œ ì˜ˆìƒì¹˜ ëª»í•œ BigQuery ë¹„ìš© ë°œìƒ ë°©ì§€
- **ë¦¬ì†ŒìŠ¤ ìµœì í™”**: íš¨ìœ¨ì ì¸ ë¦¬ì†ŒìŠ¤ ë°°ì¹˜ë¥¼ í†µí•œ ì „ì²´ì ì¸ ë¹„ìš© ì ˆê°
- **ì˜ˆì‚° ê³„íš ê°œì„ **: ì •í™•í•œ ì‹¤í–‰ ì‹œê°„ ì˜ˆì¸¡ì„ í†µí•œ ì˜ˆì‚° ê³„íš ìˆ˜ë¦½

#### 2. ê°œë°œ ìƒì‚°ì„± í–¥ìƒ
- **ë¹ ë¥¸ í”¼ë“œë°±**: ì¿¼ë¦¬ ì‹¤í–‰ ì „ì— ì„±ëŠ¥ ë¬¸ì œë¥¼ ë¯¸ë¦¬ íŒŒì•…í•˜ì—¬ ê°œë°œ ì†ë„ í–¥ìƒ
- **ìë™í™”ëœ ìµœì í™”**: ìˆ˜ë™ ìµœì í™” ê³¼ì •ì„ ìë™í™”í•˜ì—¬ ê°œë°œì ì‹œê°„ ì ˆì•½
- **í•™ìŠµ íš¨ê³¼**: ìµœì í™” ì œì•ˆì„ í†µí•œ ê°œë°œìì˜ ì¿¼ë¦¬ ì‘ì„± ê¸°ìˆ  í–¥ìƒ

#### 3. ìš´ì˜ íš¨ìœ¨ì„± ì¦ëŒ€
- **ì‘ì—… ìŠ¤ì¼€ì¤„ë§**: ì •í™•í•œ ì‹¤í–‰ ì‹œê°„ ì˜ˆì¸¡ì„ í†µí•œ íš¨ìœ¨ì ì¸ ì‘ì—… ê³„íš
- **ëª¨ë‹ˆí„°ë§ ê°•í™”**: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì„ í†µí•œ ë¬¸ì œ ì¡°ê¸° ë°œê²¬ ë° ëŒ€ì‘
- **í’ˆì§ˆ ê´€ë¦¬**: ì¼ê´€ëœ ì„±ëŠ¥ ê¸°ì¤€ì„ í†µí•œ ì¿¼ë¦¬ í’ˆì§ˆ ê´€ë¦¬

### í–¥í›„ ë°œì „ ë°©í–¥

#### 1. ê³ ë„í™”ëœ ì˜ˆì¸¡ ëª¨ë¸
- **ë”¥ëŸ¬ë‹ ëª¨ë¸ ë„ì…**: LSTM, Transformer ë“±ì„ í™œìš©í•œ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸
- **ë‹¤ì¤‘ íŠ¹ì„± ëª¨ë¸**: ì¿¼ë¦¬ ë³µì¡ë„, ë°ì´í„° ë¶„í¬, ì‹œê°„ëŒ€ ë“±ì„ ê³ ë ¤í•œ ì¢…í•© ëª¨ë¸
- **ì‹¤ì‹œê°„ í•™ìŠµ**: ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ë¥¼ í™œìš©í•œ ì‹¤ì‹œê°„ ëª¨ë¸ ì—…ë°ì´íŠ¸

#### 2. ì§€ëŠ¥í˜• ìµœì í™” ì‹œìŠ¤í…œ
- **ìë™ ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ…**: AIë¥¼ í™œìš©í•œ ì¿¼ë¦¬ ìë™ ìµœì í™”
- **ì¸ë±ìŠ¤ ì¶”ì²œ**: ë°ì´í„° íŠ¹ì„±ì„ ë¶„ì„í•œ ìµœì  ì¸ë±ìŠ¤ ìë™ ì¶”ì²œ
- **íŒŒí‹°ì…˜ ì „ëµ**: ë°ì´í„° ë¶„í¬ë¥¼ ë¶„ì„í•œ ìµœì  íŒŒí‹°ì…˜ ì „ëµ ì œì•ˆ

#### 3. í†µí•© í”Œë«í¼ êµ¬ì¶•
- **ì›¹ ëŒ€ì‹œë³´ë“œ**: ì‹œê°ì  ëª¨ë‹ˆí„°ë§ ë° ê´€ë¦¬ ì¸í„°í˜ì´ìŠ¤
- **API ìƒíƒœê³„**: ë‹¤ì–‘í•œ ë„êµ¬ì™€ì˜ ì—°ë™ì„ ìœ„í•œ RESTful API
- **ì•Œë¦¼ ì‹œìŠ¤í…œ**: Slack, Teams ë“± ë‹¤ì–‘í•œ ì±„ë„ì„ í†µí•œ ì•Œë¦¼ í†µí•©

ì´ ì‹¤í–‰ ì‹œê°„ ì¶”ì • ìë™í™” ì‹œìŠ¤í…œì„ í†µí•´ BigQuery ì‚¬ìš©ìë“¤ì´ ë”ìš± íš¨ìœ¨ì ì´ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë°ì´í„° ë¶„ì„ í™˜ê²½ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ Dry Runì„ í™œìš©í•œ ì‚¬ì „ ì˜ˆì¸¡ê³¼ ë°ì´í„° ê¸°ë°˜ ëª¨ë¸ë§ì„ í†µí•´ ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ë¥¼ ì°½ì¶œí•˜ëŠ” í˜ì‹ ì ì¸ ì ‘ê·¼ ë°©ì‹ì„ ë³´ì—¬ì£¼ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.
